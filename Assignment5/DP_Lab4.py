# -*- coding: utf-8 -*-
"""Lab4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JUNNO-hk4OnfDND7GcFdU3SglS5ZVBCb
"""

import os
import numpy as np
import tensorflow as tf
from PIL import Image
import glob
import tarfile
import gzip
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import kagglehub
import shutil

# Set random seeds for reproducibility
tf.random.set_seed(0)
np.random.seed(0)

def download_notmnist_kaggle(data_dir, subset='small'):
    """Download notMNIST dataset from Kaggle and return path to tar.gz file."""
    try:
        kaggle_path = kagglehub.dataset_download("lubaroli/notmnist")
        file_name = f'notMNIST_{subset}.tar.gz'
        kaggle_file_path = os.path.join(kaggle_path, file_name)

        if not os.path.exists(kaggle_file_path) and subset == 'small':
            kaggle_file_path = os.path.join(kaggle_path, 'notMNIST_large.tar.gz')
            file_name = 'notMNIST_large.tar.gz'

        if not os.path.exists(kaggle_file_path):
            raise FileNotFoundError(f"{file_name} not found.")

        os.makedirs(data_dir, exist_ok=True)
        dest_file_path = os.path.join(data_dir, file_name)
        shutil.copy(kaggle_file_path, dest_file_path)
        print(f"Copied {file_name} to {dest_file_path}")
        return dest_file_path
    except Exception as e:
        print("Kaggle download failed. Download manually from:")
        print("1. https://www.kaggle.com/datasets/lubaroli/notmnist")
        print(f"2. http://www.iro.umontreal.ca/~lisa/icml2007data/notMNIST_{subset}.tar.gz")
        raise RuntimeError("Kaggle download failed.") from e

def verify_and_extract_notmnist(data_dir, subset='small'):
    """Verify and extract notMNIST dataset."""
    file_path = os.path.join(data_dir, f'notMNIST_{subset}.tar.gz')
    extract_path = os.path.join(data_dir, f'notMNIST_{subset}')
    expected_sizes = {'small': 11592576, 'large': 8458043}

    if not os.path.exists(file_path):
        print(f"{file_path} not found. Attempting to download...")
        file_path = download_notmnist_kaggle(data_dir, subset)

    file_size = os.path.getsize(file_path)
    if file_size != expected_sizes.get(subset, 0):
        print(f"Warning: File {file_path} size ({file_size}) does not match expected ({expected_sizes.get(subset, 0)}).")

    try:
        with open(file_path, 'rb') as f:
            gzip.GzipFile(fileobj=f, mode='rb').read(1)
    except Exception:
        raise RuntimeError(f"Invalid gzip file: {file_path}.")

    if not os.path.exists(extract_path):
        print(f"Extracting {file_path}...")
        with tarfile.open(file_path, 'r:gz') as tar:
            tar.extractall(data_dir)
    print(f"Dataset ready at {extract_path}")
    return extract_path

def load_notmnist(data_dir, subset='small'):
    """Load notMNIST dataset from the specified directory."""
    base_path = os.path.join(data_dir, f'notMNIST_{subset}')
    data, labels = [], []
    label_map = {chr(65+i): i for i in range(10)}

    for letter in 'ABCDEFGHIJ':
        folder = os.path.join(base_path, letter)
        if not os.path.exists(folder):
            print(f"Warning: Folder {folder} not found.")
            continue
        img_paths = glob.glob(os.path.join(folder, '*.png'))
        print(f"Found {len(img_paths)} images in folder {letter}")
        for img_path in img_paths:
            try:
                img = Image.open(img_path).convert('L')
                img = np.array(img) / 255.0
                data.append(img.flatten())
                labels.append(label_map[letter])
            except Exception as e:
                print(f"Error loading {img_path}: {e}")
                continue

    data = np.array(data)
    labels = np.array(labels)
    print(f"Loaded {len(data)} images with {len(labels)} labels")
    if len(data) == 0 or len(labels) == 0:
        raise ValueError("No data loaded.")
    return data, labels

def prepare_dataset(data_dir, subset='small', batch_size=32):
    """Load and prepare notMNIST dataset for training."""
    verify_and_extract_notmnist(data_dir, subset=subset)
    X, y = load_notmnist(data_dir, subset=subset)
    y = tf.keras.utils.to_categorical(y, 10)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    print(f"Train set: {X_train.shape}, Test set: {X_test.shape}")
    X_train = X_train.reshape(-1, 1, 784)
    X_test = X_test.reshape(-1, 1, 784)
    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1000).batch(batch_size)
    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)
    return train_dataset, test_dataset

class BasicGRU(tf.keras.layers.Layer):
    def __init__(self, units, **kwargs):
        super().__init__(**kwargs)
        self.units = units
        self.state_size = units
        self.z_kernel = tf.keras.layers.Dense(units, activation='sigmoid')
        self.r_kernel = tf.keras.layers.Dense(units, activation='sigmoid')
        self.s_kernel = tf.keras.layers.Dense(units, activation='tanh')

    def build(self, input_shape):
        # Input shape for RNN cell: (batch, features)
        # State shape: (batch, units)
        concat_shape = (input_shape[0], input_shape[-1] + self.units)
        self.z_kernel.build(concat_shape)
        self.r_kernel.build(concat_shape)
        reset_shape = (input_shape[0], input_shape[-1] + self.units)
        self.s_kernel.build(reset_shape)
        self.built = True

    def call(self, inputs, states):
        h = states[0]
        concat = tf.concat([h, inputs], axis=-1)
        z = self.z_kernel(concat)
        r = self.r_kernel(concat)
        concat_reset = tf.concat([r * h, inputs], axis=-1)
        s_tilde = self.s_kernel(concat_reset)
        h_new = (1 - z) * h + z * s_tilde
        return h_new, [h_new]

    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):
        if dtype is None:
            dtype = tf.float32
        return [tf.zeros((batch_size, self.units), dtype=dtype)]

class BasicMGU(tf.keras.layers.Layer):
    def __init__(self, units, **kwargs):
        super().__init__(**kwargs)
        self.units = units
        self.state_size = units
        self.f_kernel = tf.keras.layers.Dense(units, activation='sigmoid')
        self.s_kernel = tf.keras.layers.Dense(units, activation='tanh')

    def build(self, input_shape):
        # Input shape for RNN cell: (batch, features)
        # State shape: (batch, units)
        concat_shape = (input_shape[0], input_shape[-1] + self.units)
        self.f_kernel.build(concat_shape)
        reset_shape = (input_shape[0], input_shape[-1] + self.units)
        self.s_kernel.build(reset_shape)
        self.built = True

    def call(self, inputs, states):
        h = states[0]
        concat = tf.concat([h, inputs], axis=-1)
        f = self.f_kernel(concat)
        concat_reset = tf.concat([f * h, inputs], axis=-1)
        s_tilde = self.s_kernel(concat_reset)
        h_new = (1 - f) * h + f * s_tilde
        return h_new, [h_new]

    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):
        if dtype is None:
            dtype = tf.float32
        return [tf.zeros((batch_size, self.units), dtype=dtype)]

def train_model(model, train_dataset, test_dataset, epochs):
    """Train the model and return training history."""
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    history = model.fit(train_dataset, epochs=epochs, validation_data=test_dataset, verbose=1)
    train_err = [1 - acc for acc in history.history['accuracy']]
    test_err = [1 - acc for acc in history.history['val_accuracy']]
    return train_err, test_err

def plot_curves(gru_errors, mgu_errors, output_dir='./plots'):
    """Plot training and test error curves for GRU and MGU."""
    os.makedirs(output_dir, exist_ok=True)
    epochs = range(1, len(gru_errors[0][0]) + 1)
    plt.figure(figsize=(10, 6))
    for model_name, errors in [('GRU', gru_errors), ('MGU', mgu_errors)]:
        train_errors = np.mean([trial[0] for trial in errors], axis=0)
        test_errors = np.mean([trial[1] for trial in errors], axis=0)
        plt.plot(epochs, train_errors, label=f'{model_name} Train Error')
        plt.plot(epochs, test_errors, linestyle='--', label=f'{model_name} Test Error')
    plt.xlabel('Epoch')
    plt.ylabel('Classification Error (1 - Accuracy)')
    plt.title('Training vs. Test Error Curves')
    plt.legend()
    plt.grid(True)
    plt.savefig(os.path.join(output_dir, 'error_curves.png'))
    plt.close()

if __name__ == "__main__":
    data_dir = './notMNIST'
    subset = 'small'
    batch_size = 32
    units, epochs, trials = 128, 10, 3

    try:
        train_dataset, test_dataset = prepare_dataset(data_dir, subset=subset, batch_size=batch_size)
        print("Dataset prepared successfully!")

        gru_errors, mgu_errors = [], []
        for trial in range(trials):
            print(f"\nTrial {trial + 1}/{trials}")
            gru_model = tf.keras.Sequential([
                tf.keras.Input(shape=(1, 784)),
                tf.keras.layers.RNN(BasicGRU(units)),
                tf.keras.layers.Dense(10, activation='softmax')
            ])
            gru_train_err, gru_test_err = train_model(gru_model, train_dataset, test_dataset, epochs)
            gru_errors.append((gru_train_err, gru_test_err))
            print(f"GRU Trial {trial + 1} - Final Test Error: {gru_test_err[-1]:.4f}")

            mgu_model = tf.keras.Sequential([
                tf.keras.Input(shape=(1, 784)),
                tf.keras.layers.RNN(BasicMGU(units)),
                tf.keras.layers.Dense(10, activation='softmax')
            ])
            mgu_train_err, mgu_test_err = train_model(mgu_model, train_dataset, test_dataset, epochs)
            mgu_errors.append((mgu_train_err, mgu_test_err))
            print(f"MGU Trial {trial + 1} - Final Test Error: {mgu_test_err[-1]:.4f}")

        plot_curves(gru_errors, mgu_errors)

        gru_avg_test_error = np.mean([err[1][-1] for err in gru_errors])
        mgu_avg_test_error = np.mean([err[1][-1] for err in mgu_errors])
        gru_std_test_error = np.std([err[1][-1] for err in gru_errors])
        mgu_std_test_error = np.std([err[1][-1] for err in mgu_errors])

        print("\nFinal Results:")
        print(f"GRU Average Test Error: {gru_avg_test_error:.4f} ± {gru_std_test_error:.4f}")
        print(f"MGU Average Test Error: {mgu_avg_test_error:.4f} ± {mgu_std_test_error:.4f}")

    except Exception as e:
        print(f"Error: {e}")