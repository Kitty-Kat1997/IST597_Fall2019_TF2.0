# -*- coding: utf-8 -*-
"""Lab3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JUNNO-hk4OnfDND7GcFdU3SglS5ZVBCb
"""

import tensorflow as tf
import numpy as np

# Load and preprocess Fashion MNIST data
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()
x_train = np.expand_dims(x_train.astype(np.float32) / 255.0, -1)
x_test = np.expand_dims(x_test.astype(np.float32) / 255.0, -1)

train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(5000).batch(100)
test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(100)

# ---------------- Custom Normalization Functions ----------------

def batch_norm(x, gamma, beta, epsilon=1e-5):
    axes = list(range(len(x.shape) - 1))
    mean = tf.reduce_mean(x, axis=axes, keepdims=True)
    var = tf.reduce_mean(tf.square(x - mean), axis=axes, keepdims=True)
    x_norm = (x - mean) / tf.sqrt(var + epsilon)
    return gamma * x_norm + beta

def weight_norm(v, g, axis=None, epsilon=1e-5):
    v_norm = tf.sqrt(tf.reduce_sum(tf.square(v), axis=axis, keepdims=True) + epsilon)
    return (g / v_norm) * v

def layer_norm(x, gamma, beta, epsilon=1e-5):
    mean = tf.reduce_mean(x, axis=-1, keepdims=True)
    var = tf.reduce_mean(tf.square(x - mean), axis=-1, keepdims=True)
    x_norm = (x - mean) / tf.sqrt(var + epsilon)
    return gamma * x_norm + beta

# ---------------- CNN Model ----------------

class CNN(tf.keras.Model):
    def __init__(self, num_classes=10, norm_type='none'):
        super(CNN, self).__init__()
        self.norm_type = norm_type
        self.pool1 = tf.keras.layers.MaxPooling2D(pool_size=2)
        self.flatten = tf.keras.layers.Flatten()
        self.dense = tf.keras.layers.Dense(num_classes)

        if norm_type == 'weight':
            self.v = tf.Variable(tf.random.normal([3, 3, 1, 32]), trainable=True)
            self.g = tf.Variable(tf.ones([1, 1, 1, 32]), trainable=True)
        else:
            self.conv1 = tf.keras.layers.Conv2D(32, 3, padding='same', use_bias=True)

        if norm_type == 'batch':
            self.gamma = tf.Variable(tf.ones([1, 1, 1, 32]), trainable=True)
            self.beta = tf.Variable(tf.zeros([1, 1, 1, 32]), trainable=True)
        elif norm_type == 'layer':
            self.gamma = tf.Variable(tf.ones([32]), trainable=True)
            self.beta = tf.Variable(tf.zeros([32]), trainable=True)

    def call(self, x, training=False):
        if self.norm_type == 'weight':
            w = weight_norm(self.v, self.g, axis=[0, 1, 2])
            x = tf.nn.conv2d(x, w, strides=1, padding='SAME')
        else:
            x = self.conv1(x)

        if self.norm_type == 'batch':
            x = batch_norm(x, self.gamma, self.beta)
        elif self.norm_type == 'layer':
            reshaped = tf.reshape(x, [-1, x.shape[-1]])
            normed = layer_norm(reshaped, self.gamma, self.beta)
            x = tf.reshape(normed, tf.shape(x))

        x = tf.nn.relu(x)
        x = self.pool1(x)
        x = self.flatten(x)
        return self.dense(x)

# ---------------- Training Functions ----------------

loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

def train_model(model, epochs=2):
    optimizer = tf.keras.optimizers.Adam()
    for epoch in range(epochs):
        for images, labels in train_dataset:
            with tf.GradientTape() as tape:
                logits = model(images, training=True)
                loss = loss_fn(labels, logits)
            grads = tape.gradient(loss, model.trainable_variables)
            optimizer.apply_gradients(zip(grads, model.trainable_variables))

def test_model(model):
    accuracy = tf.keras.metrics.SparseCategoricalAccuracy()
    for images, labels in test_dataset:
        logits = model(images, training=False)
        accuracy.update_state(labels, logits)
    return accuracy.result().numpy()

# ---------------- Run All Models ----------------

models = {
    'NoNorm': CNN(norm_type='none'),
    'BatchNorm': CNN(norm_type='batch'),
    'LayerNorm': CNN(norm_type='layer'),
    'WeightNorm': CNN(norm_type='weight'),
}

results = {}
for name, model in models.items():
    print(f"\nTraining with {name}...")
    train_model(model, epochs=2)
    acc = test_model(model)
    results[name] = acc
    print(f"{name} Test Accuracy: {acc:.4f}")

# ---------------- Compare with TensorFlow Built-in BN ----------------

print("\nComparing custom BatchNorm with tf.keras.layers.BatchNormalization...")
sample = tf.random.normal([32, 28, 28, 32])
gamma = tf.ones([1, 1, 1, 32])
beta = tf.zeros([1, 1, 1, 32])
custom_out = batch_norm(sample, gamma, beta)

tf_bn = tf.keras.layers.BatchNormalization()
tf_out = tf_bn(sample, training=True)

diff = tf.reduce_mean(tf.abs(custom_out - tf_out)).numpy()
print(f"Mean absolute difference: {diff:.6f}")