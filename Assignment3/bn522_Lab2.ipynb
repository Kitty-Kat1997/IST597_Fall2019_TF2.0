{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHi3seuXfwSj",
        "outputId": "3dd2bf13-112e-4cc2-9d80-1ebed655a75a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, optimizers, losses\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize and reshape images\n",
        "train_images = train_images.reshape(-1, 784).astype('float32') / 255.0\n",
        "test_images = test_images.reshape(-1, 784).astype('float32') / 255.0\n",
        "\n",
        "# Generate permuted tasks\n",
        "num_tasks = 10\n",
        "tasks_train_images = []\n",
        "tasks_test_images = []\n",
        "for task in range(num_tasks):\n",
        "    permutation = np.random.permutation(784)\n",
        "    tasks_train_images.append(train_images[:, permutation])\n",
        "    tasks_test_images.append(test_images[:, permutation])\n",
        "\n",
        "# Define MLP model\n",
        "class MLP(tf.keras.Model):\n",
        "    def __init__(self, depth, dropout_prob, optimizer_type):\n",
        "        super(MLP, self).__init__()\n",
        "        self.depth = depth\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.hidden_layers = [layers.Dense(256, activation='relu') for _ in range(depth - 1)]\n",
        "        self.dropout = layers.Dropout(dropout_prob)\n",
        "        self.output_layer = layers.Dense(10, activation='softmax')\n",
        "\n",
        "        if optimizer_type == \"SGD\":\n",
        "            self.optimizer = optimizers.SGD()\n",
        "        elif optimizer_type == \"Adam\":\n",
        "            self.optimizer = optimizers.Adam()\n",
        "        elif optimizer_type == \"RMSProp\":\n",
        "            self.optimizer = optimizers.RMSprop()\n",
        "        else:\n",
        "            raise ValueError(\"Optimizer not supported\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = inputs\n",
        "        for layer in self.hidden_layers:\n",
        "            x = layer(x)\n",
        "            if training:\n",
        "                x = self.dropout(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "    def compute_loss(self, logits, labels, loss_type):\n",
        "        if loss_type == \"NLL\":\n",
        "            loss = losses.SparseCategoricalCrossentropy()(labels, logits)\n",
        "        elif loss_type == \"L1\":\n",
        "            loss = tf.reduce_mean(tf.abs(tf.one_hot(labels, depth=10) - logits))\n",
        "        elif loss_type == \"L2\":\n",
        "            loss = tf.reduce_mean(tf.square(tf.one_hot(labels, depth=10) - logits))\n",
        "        elif loss_type == \"L1+L2\":\n",
        "            loss = tf.reduce_mean(tf.abs(tf.one_hot(labels, depth=10) - logits)) + \\\n",
        "                   tf.reduce_mean(tf.square(tf.one_hot(labels, depth=10) - logits))\n",
        "        else:\n",
        "            raise ValueError(\"Loss type not supported\")\n",
        "        return loss\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_images, train_labels, epochs, dropout_prob, loss_type):\n",
        "    for epoch in range(epochs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(train_images, training=True)\n",
        "            loss = model.compute_loss(logits, train_labels, loss_type)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return model\n",
        "\n",
        "# Testing function\n",
        "def test_model(model, test_images, test_labels):\n",
        "    logits = model(test_images, training=False)\n",
        "    predictions = tf.argmax(logits, axis=1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, test_labels), tf.float32))\n",
        "    return accuracy.numpy()\n",
        "\n",
        "# Sequential training and evaluation\n",
        "def sequential_training(model, tasks_train_images, tasks_train_labels, tasks_test_images, tasks_test_labels, epochs_per_task, dropout_prob, loss_type):\n",
        "    num_tasks = len(tasks_train_images)\n",
        "    R = np.zeros((num_tasks, num_tasks))  # Result matrix\n",
        "\n",
        "    for task_index in range(num_tasks):\n",
        "        epochs = 50 if task_index == 0 else 20  # 50 epochs for Task A, 20 for others\n",
        "        model = train_model(model, tasks_train_images[task_index], train_labels, epochs, dropout_prob, loss_type)\n",
        "\n",
        "        # Test on all seen tasks\n",
        "        for test_task in range(task_index + 1):\n",
        "            accuracy = test_model(model, tasks_test_images[test_task], test_labels)\n",
        "            R[task_index][test_task] = accuracy\n",
        "\n",
        "    return R\n",
        "\n",
        "# Calculate ACC and BWT\n",
        "def calculate_ACC(R):\n",
        "    T = R.shape[0]\n",
        "    return np.mean(R[-1, :])\n",
        "\n",
        "def calculate_BWT(R):\n",
        "    T = R.shape[0]\n",
        "    bwt = 0\n",
        "    for i in range(T - 1):\n",
        "        bwt += R[-1, i] - R[i, i]\n",
        "    return bwt / (T - 1)\n",
        "\n",
        "# Optional: Calculate TBWT and CBWT\n",
        "def calculate_TBWT(R, task_index):\n",
        "    return R[-1, task_index] - R[task_index, task_index]\n",
        "\n",
        "def calculate_CBWT(R):\n",
        "    T = R.shape[0]\n",
        "    cbwt = 0\n",
        "    for i in range(T):\n",
        "        cbwt += calculate_TBWT(R, i)\n",
        "    return cbwt / T\n",
        "\n",
        "# Experiment with different configurations\n",
        "loss_types = [\"NLL\", \"L1\", \"L2\", \"L1+L2\"]\n",
        "optimizers_list = [\"SGD\", \"Adam\", \"RMSProp\"]\n",
        "depths = [2, 3, 4]\n",
        "dropout_rates = [0.2, 0.4]\n",
        "\n",
        "results = []\n",
        "\n",
        "for loss_type in loss_types:\n",
        "    for optimizer_type in optimizers_list:\n",
        "        for depth in depths:\n",
        "            for dropout_prob in dropout_rates:\n",
        "                model = MLP(depth=depth, dropout_prob=dropout_prob, optimizer_type=optimizer_type)\n",
        "                R = sequential_training(model, tasks_train_images, train_labels, tasks_test_images, test_labels, epochs_per_task=20, dropout_prob=dropout_prob, loss_type=loss_type)\n",
        "                ACC = calculate_ACC(R)\n",
        "                BWT = calculate_BWT(R)\n",
        "                results.append((loss_type, optimizer_type, depth, dropout_prob, ACC, BWT))\n",
        "                print(f\"Loss: {loss_type}, Optimizer: {optimizer_type}, Depth: {depth}, Dropout: {dropout_prob}, ACC: {ACC}, BWT: {BWT}\")\n",
        "\n",
        "# Plot validation results\n",
        "def plot_validation_results(all_accuracy_results):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i, accuracies in enumerate(all_accuracy_results):\n",
        "        plt.plot(accuracies, label=f'Task {i+1}')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Validation Accuracy')\n",
        "    plt.title('Validation Accuracy Over Tasks')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Assuming all_accuracy_results is a list of lists containing validation accuracies for each task\n",
        "plot_validation_results(all_accuracy_results)"
      ]
    }
  ]
}